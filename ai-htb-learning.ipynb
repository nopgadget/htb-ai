{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b912a171",
   "metadata": {},
   "source": [
    "# Python Libraries for AI\n",
    "\n",
    "## Scikit-learn\n",
    "- Built on numpy, scipy, matplotlib\n",
    "- Does supervised learning, unsupervised, model selection & eval, data processing\n",
    "\n",
    "Data Preprocessing\n",
    "- tools to transform raw data into suitable format\n",
    "- Feature scaling. StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9c077",
   "metadata": {},
   "source": [
    "OneHotEncoder: Creates binary column for each category\n",
    "LabelEncoder: Unique int for each cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c994e",
   "metadata": {},
   "source": [
    "SimpleImputer: Replace missing values with specified strat (mean, median, most frequent, etc)\n",
    "KNNImputer: Imputes missing values with k-Nearest neighbors alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263067d",
   "metadata": {},
   "source": [
    "Scikit-learn has tools for selecting best model & evaluating performance\n",
    "Splitting data into training & testing sets important for evaluating model's generalization ability for unseen data\n",
    "\n",
    "Cross-validation more robust eval by splitting data into folds and testing/training on diff combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3392d50",
   "metadata": {},
   "source": [
    "Metrics to eval model performance\n",
    "\n",
    "- accuracy_score: Class tasks\n",
    "- mean_squared_error: Regression tasks\n",
    "- precision_score, recall_score, f1_score: Class tasks with imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c46a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c657b8c",
   "metadata": {},
   "source": [
    "Model Training & Prediction\n",
    "Consistent API for training & predicting w/ diff models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c172dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=1.0)\n",
    "\n",
    "# Train model using fit() method with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data using predict()\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346c37d",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "FOSS ml lib developed by FB AI Research Lab. Framework for building & deploying various types of ML models, including Deep Learning models\n",
    "\n",
    "Features\n",
    "- Deep Learning: Excels at DL, can develop CNN with multiple layers & architectures\n",
    "- Dynamic Computational Graphs: Unlike static comp graphs like in TensorFlow, uses DCG which allow for more flexible & intuitive model building/debugging\n",
    "- GPU Support: GPU accel, speeds up training process for compu intensive models\n",
    "- TorchVision Integration: Library integrated that provides user-friendly interface for image datasets, pre-trained models, common image transformations\n",
    "- Auto Differentiation: Uses autograd to auto compute gradients, simplifying process of backpropagation\n",
    "- Community/Ecosystem: Large community, rich ecosystem tools, libraries, resources\n",
    "\n",
    "Dynamic Computation Graph: created on fly during forward pass, allowing more flexible & dynamic model building. Easier to implement complex/nonlinear models\n",
    "Tensors: multi-dimensional array hold data being processed. Can be const, var, placeholders. PyTorch tensors similar to numpy arrays but can run on GPU for faster compu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0,2.0,3.0])\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to('cuda')\n",
    "\n",
    "# torch.nn contains various layers/modules for constructing NN\n",
    "# Sequential API allows building models layer by layer, adding each sequentially\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Module class provides more flex for building complex models with nonlinear topologies, shared layers, multiple input/output\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(784,128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128,10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d528117",
   "metadata": {},
   "source": [
    "Training and Eval\n",
    "\n",
    "Optimizers: algs that adjust model's params during training to minimize loss function. Pytorch offers various\n",
    "- Adam\n",
    "- SGD (Stcochastic Gradient Descent)\n",
    "- RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ae389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046d5fe",
   "metadata": {},
   "source": [
    "Loss func measure diff between model's predict and actual target vals. Pytorch provides various loss func\n",
    "- CrossEntropyLoss: for multi-class classification\n",
    "- BCEWithLogitsLoss: for binary classif\n",
    "- MSELoss: for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b031bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d136745",
   "metadata": {},
   "source": [
    "Metrics eval model's preform during test/train\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f23e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    _, predicted = torch.max(output,1)\n",
    "    correct = (predicted == target).sum().item()\n",
    "    return correct / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop updates models params based on training data\n",
    "import torch\n",
    "\n",
    "epochs = 10\n",
    "num_batches = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(num_batches):\n",
    "        # Get batch of data\n",
    "        x_batch, y_batch = get_batch(batch)\n",
    "\n",
    "        # Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # Back pass & optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch+1}/{num_batches}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preprocessing\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx],self.labels[idx]\n",
    "    \n",
    "dataset = CustomDataset(data,labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving and Loading\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model = CustomModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval() # Set model to eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c616094",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Collections of data points used for analysis & model training. Data Preprocessing is crucial in ML pipeline, transforming raw data into suitable for alg to process effectively\n",
    "\n",
    "Forms of datasets\n",
    "- Tabular Data: Organized into tables w/ rows and columns, common in spreadsheets/dbs\n",
    "- Image Data: Sets of images, numerically as pixel arrays\n",
    "- Text Data: Unstructured data, sentences, paragraphs, full documents\n",
    "- Time Series Data: Sequential data points collected over time, emphasizing temporal patterns\n",
    "\n",
    "Quality of dataset is important\n",
    "- Model Accuracy: Quality datasets = more accurate models. Poor quality like noisy, incomplete, biased leads to poorer model performance\n",
    "- Generalization: Carefully curated allows effective generalization for unseen data. Minimized overfitting and ensures consistent performance in real-world\n",
    "- Efficiency: Clean, well-prepared data reduces train time & compu demands, streamlining entire process\n",
    "- Reliability: Reliable dataset leads to trustworthy insight/decision. In critical domains like healthcare/finance, data quality affects dependability of results.\n",
    "\n",
    "## What Makes a Dataset Good\n",
    "\n",
    "- Relevance\n",
    "- Completeness\n",
    "- Consistency (format)\n",
    "- Quality: accurate, free from errors; errors can arise from data collection, entry, or transmission issues\n",
    "- Representativeness\n",
    "- Balance: Especially important for classification. Imbalanced leads to bias that performs poorly on minority classes. Techniques (oversampling,undersampling,synthetic data) can help balance\n",
    "- Size\n",
    "\n",
    "## Dataset\n",
    "\n",
    "demo_dataset.csv is a csv file containing network log entries. analyzing entries allows one to simulate various network scenarios useful for developing/evaluating IDS\n",
    "\n",
    "### Structure\n",
    "\n",
    "- log_id: Unique ID for each entry\n",
    "- source_ip\n",
    "- destination_port\n",
    "- protocol\n",
    "- bytes_transferred\n",
    "- threat_level : Indicator of severity. 0 normal, 1 low-threat, 2 high-threat\n",
    "\n",
    "### Challenges & Considerations\n",
    "\n",
    "- Dataset contains mix of numerical and categorical\n",
    "- Missing values and invalid entries in some columns, requiring data cleaning\n",
    "- Some numeric columns may contain non-numeric strings, which must be converted/removed\n",
    "- Threat_level column has unknown values that must be standardized/addressed during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c2bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pandas Dataframe is flexible, 2D labeled data structure that supports operations for data exploring/preprocessing\n",
    "# advantages: labeled axes, heterogeneous data handling, integration with other libraries\n",
    "\n",
    "data = pd.read_csv('./demo_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f514a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   log_id       source_ip destination_port protocol bytes_transferred  \\\n",
      "0      10      10.0.0.100      STRING_PORT      FTP              4096   \n",
      "1      12  172.16.254.100              110     POP3          NEGATIVE   \n",
      "2      27  172.16.254.200              110     POP3       NON_NUMERIC   \n",
      "3       1   192.168.1.100               80     HTTP              1024   \n",
      "4       2    192.168.1.81               53      TLS              9765   \n",
      "\n",
      "  threat_level  \n",
      "0            ?  \n",
      "1            1  \n",
      "2            1  \n",
      "3            0  \n",
      "4            0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   log_id             100 non-null    int64 \n",
      " 1   source_ip          99 non-null     object\n",
      " 2   destination_port   99 non-null     object\n",
      " 3   protocol           100 non-null    object\n",
      " 4   bytes_transferred  100 non-null    object\n",
      " 5   threat_level       100 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.8+ KB\n",
      "None\n",
      "log_id               0\n",
      "source_ip            1\n",
      "destination_port     1\n",
      "protocol             0\n",
      "bytes_transferred    0\n",
      "threat_level         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Various ops to understand structure, anomalies, determine cleaning/transformations needed\n",
    "\n",
    "# First few rows of dataset\n",
    "print(data.head())\n",
    "\n",
    "# Summary of column data types and non-null counts\n",
    "# Shows dataset shape, column names, data types, how many entries for each. Early detection of columns w/ unexpected/missing data\n",
    "print(data.info())\n",
    "\n",
    "# Identify col with missing vals\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901b8498",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htb-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
