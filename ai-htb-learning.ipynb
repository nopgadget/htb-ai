{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b912a171",
   "metadata": {},
   "source": [
    "# Python Libraries for AI\n",
    "\n",
    "## Scikit-learn\n",
    "- Built on numpy, scipy, matplotlib\n",
    "- Does supervised learning, unsupervised, model selection & eval, data processing\n",
    "\n",
    "Data Preprocessing\n",
    "- tools to transform raw data into suitable format\n",
    "- Feature scaling. StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9c077",
   "metadata": {},
   "source": [
    "OneHotEncoder: Creates binary column for each category\n",
    "LabelEncoder: Unique int for each cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "X_encoded = encoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81c994e",
   "metadata": {},
   "source": [
    "SimpleImputer: Replace missing values with specified strat (mean, median, most frequent, etc)\n",
    "KNNImputer: Imputes missing values with k-Nearest neighbors alg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1c6c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d263067d",
   "metadata": {},
   "source": [
    "Scikit-learn has tools for selecting best model & evaluating performance\n",
    "Splitting data into training & testing sets important for evaluating model's generalization ability for unseen data\n",
    "\n",
    "Cross-validation more robust eval by splitting data into folds and testing/training on diff combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122fe207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3392d50",
   "metadata": {},
   "source": [
    "Metrics to eval model performance\n",
    "\n",
    "- accuracy_score: Class tasks\n",
    "- mean_squared_error: Regression tasks\n",
    "- precision_score, recall_score, f1_score: Class tasks with imbalanced classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c46a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c657b8c",
   "metadata": {},
   "source": [
    "Model Training & Prediction\n",
    "Consistent API for training & predicting w/ diff models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c172dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(C=1.0)\n",
    "\n",
    "# Train model using fit() method with training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on new data using predict()\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f346c37d",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "FOSS ml lib developed by FB AI Research Lab. Framework for building & deploying various types of ML models, including Deep Learning models\n",
    "\n",
    "Features\n",
    "- Deep Learning: Excels at DL, can develop CNN with multiple layers & architectures\n",
    "- Dynamic Computational Graphs: Unlike static comp graphs like in TensorFlow, uses DCG which allow for more flexible & intuitive model building/debugging\n",
    "- GPU Support: GPU accel, speeds up training process for compu intensive models\n",
    "- TorchVision Integration: Library integrated that provides user-friendly interface for image datasets, pre-trained models, common image transformations\n",
    "- Auto Differentiation: Uses autograd to auto compute gradients, simplifying process of backpropagation\n",
    "- Community/Ecosystem: Large community, rich ecosystem tools, libraries, resources\n",
    "\n",
    "Dynamic Computation Graph: created on fly during forward pass, allowing more flexible & dynamic model building. Easier to implement complex/nonlinear models\n",
    "Tensors: multi-dimensional array hold data being processed. Can be const, var, placeholders. PyTorch tensors similar to numpy arrays but can run on GPU for faster compu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0,2.0,3.0])\n",
    "if torch.cuda.is_available():\n",
    "    x = x.to('cuda')\n",
    "\n",
    "# torch.nn contains various layers/modules for constructing NN\n",
    "# Sequential API allows building models layer by layer, adding each sequentially\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784,128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "\n",
    "# Module class provides more flex for building complex models with nonlinear topologies, shared layers, multiple input/output\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.layer1 = nn.Linear(784,128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(128,10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self,x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    \n",
    "model = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d528117",
   "metadata": {},
   "source": [
    "Training and Eval\n",
    "\n",
    "Optimizers: algs that adjust model's params during training to minimize loss function. Pytorch offers various\n",
    "- Adam\n",
    "- SGD (Stcochastic Gradient Descent)\n",
    "- RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ae389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8046d5fe",
   "metadata": {},
   "source": [
    "Loss func measure diff between model's predict and actual target vals. Pytorch provides various loss func\n",
    "- CrossEntropyLoss: for multi-class classification\n",
    "- BCEWithLogitsLoss: for binary classif\n",
    "- MSELoss: for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b031bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d136745",
   "metadata": {},
   "source": [
    "Metrics eval model's preform during test/train\n",
    "- Accuracy\n",
    "- Precision\n",
    "- Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f23e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, target):\n",
    "    _, predicted = torch.max(output,1)\n",
    "    correct = (predicted == target).sum().item()\n",
    "    return correct / len(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399c75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop updates models params based on training data\n",
    "import torch\n",
    "\n",
    "epochs = 10\n",
    "num_batches = 100\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in range(num_batches):\n",
    "        # Get batch of data\n",
    "        x_batch, y_batch = get_batch(batch)\n",
    "\n",
    "        # Forward Pass\n",
    "        y_pred = model(x_batch)\n",
    "\n",
    "        # Calc loss\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "        # Back pass & optim\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Batch [{batch+1}/{num_batches}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841b3e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loading & Preprocessing\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,data,labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx],self.labels[idx]\n",
    "    \n",
    "dataset = CustomDataset(data,labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6acd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Saving and Loading\n",
    "\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n",
    "model = CustomModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval() # Set model to eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c616094",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Collections of data points used for analysis & model training. Data Preprocessing is crucial in ML pipeline, transforming raw data into suitable for alg to process effectively\n",
    "\n",
    "Forms of datasets\n",
    "- Tabular Data: Organized into tables w/ rows and columns, common in spreadsheets/dbs\n",
    "- Image Data: Sets of images, numerically as pixel arrays\n",
    "- Text Data: Unstructured data, sentences, paragraphs, full documents\n",
    "- Time Series Data: Sequential data points collected over time, emphasizing temporal patterns\n",
    "\n",
    "Quality of dataset is important\n",
    "- Model Accuracy: Quality datasets = more accurate models. Poor quality like noisy, incomplete, biased leads to poorer model performance\n",
    "- Generalization: Carefully curated allows effective generalization for unseen data. Minimized overfitting and ensures consistent performance in real-world\n",
    "- Efficiency: Clean, well-prepared data reduces train time & compu demands, streamlining entire process\n",
    "- Reliability: Reliable dataset leads to trustworthy insight/decision. In critical domains like healthcare/finance, data quality affects dependability of results.\n",
    "\n",
    "## What Makes a Dataset Good\n",
    "\n",
    "- Relevance\n",
    "- Completeness\n",
    "- Consistency (format)\n",
    "- Quality: accurate, free from errors; errors can arise from data collection, entry, or transmission issues\n",
    "- Representativeness\n",
    "- Balance: Especially important for classification. Imbalanced leads to bias that performs poorly on minority classes. Techniques (oversampling,undersampling,synthetic data) can help balance\n",
    "- Size\n",
    "\n",
    "## Dataset\n",
    "\n",
    "demo_dataset.csv is a csv file containing network log entries. analyzing entries allows one to simulate various network scenarios useful for developing/evaluating IDS\n",
    "\n",
    "### Structure\n",
    "\n",
    "- log_id: Unique ID for each entry\n",
    "- source_ip\n",
    "- destination_port\n",
    "- protocol\n",
    "- bytes_transferred\n",
    "- threat_level : Indicator of severity. 0 normal, 1 low-threat, 2 high-threat\n",
    "\n",
    "### Challenges & Considerations\n",
    "\n",
    "- Dataset contains mix of numerical and categorical\n",
    "- Missing values and invalid entries in some columns, requiring data cleaning\n",
    "- Some numeric columns may contain non-numeric strings, which must be converted/removed\n",
    "- Threat_level column has unknown values that must be standardized/addressed during preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65c2bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# pandas Dataframe is flexible, 2D labeled data structure that supports operations for data exploring/preprocessing\n",
    "# advantages: labeled axes, heterogeneous data handling, integration with other libraries\n",
    "\n",
    "data = pd.read_csv('./demo_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f514a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   log_id       source_ip destination_port protocol bytes_transferred  \\\n",
      "0      10      10.0.0.100      STRING_PORT      FTP              4096   \n",
      "1      12  172.16.254.100              110     POP3          NEGATIVE   \n",
      "2      27  172.16.254.200              110     POP3       NON_NUMERIC   \n",
      "3       1   192.168.1.100               80     HTTP              1024   \n",
      "4       2    192.168.1.81               53      TLS              9765   \n",
      "\n",
      "  threat_level  \n",
      "0            ?  \n",
      "1            1  \n",
      "2            1  \n",
      "3            0  \n",
      "4            0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   log_id             100 non-null    int64 \n",
      " 1   source_ip          99 non-null     object\n",
      " 2   destination_port   99 non-null     object\n",
      " 3   protocol           100 non-null    object\n",
      " 4   bytes_transferred  100 non-null    object\n",
      " 5   threat_level       100 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 4.8+ KB\n",
      "None\n",
      "log_id               0\n",
      "source_ip            1\n",
      "destination_port     1\n",
      "protocol             0\n",
      "bytes_transferred    0\n",
      "threat_level         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Various ops to understand structure, anomalies, determine cleaning/transformations needed\n",
    "\n",
    "# First few rows of dataset\n",
    "print(data.head())\n",
    "\n",
    "# Summary of column data types and non-null counts\n",
    "# Shows dataset shape, column names, data types, how many entries for each. Early detection of columns w/ unexpected/missing data\n",
    "print(data.info())\n",
    "\n",
    "# Identify col with missing vals\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b8498",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "- Data Cleaning: Missing values, Duplicates, Smoothing noisy data\n",
    "- Data Transformation: Normalizing, encoding, scaling, reducing data\n",
    "- Data Integration: Merging/Aggregating from multiple sources\n",
    "- Data Formatting: Converting types and reshaping data structures\n",
    "\n",
    "Addresses inconsistencies, missing vals, outliers, noise, feature scaling, improving accuracy, efficiency, robustness of ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e567dfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    log_id       source_ip destination_port protocol bytes_transferred  \\\n",
      "40      41      10.0.0.300               25     SMTP              4096   \n",
      "51      52    10.10.10.450      STRING_PORT      FTP              4096   \n",
      "55      56             NaN               53      DNS              1024   \n",
      "57      58   192.168.1.475              NaN      UDP              2048   \n",
      "63      64      MISSING_IP               53      DNS              1024   \n",
      "65      66   192.168.1.600      UNUSED_PORT      UDP              2048   \n",
      "71      72      MISSING_IP               53      DNS              1024   \n",
      "74      75    172.16.1.400               80     HTTP              1024   \n",
      "82      83    172.16.1.450               80     HTTP              1024   \n",
      "87      88      MISSING_IP               53      DNS              1024   \n",
      "88      89    10.10.10.700              443      TLS               512   \n",
      "92      93      INVALID_IP              110     POP3              4096   \n",
      "93      94  192.168.1.1050               53      DNS       NON_NUMERIC   \n",
      "95      96      MISSING_IP               25     SMTP              4096   \n",
      "97      98  192.168.1.1100      UNUSED_PORT      UDP              2048   \n",
      "\n",
      "   threat_level  \n",
      "40            0  \n",
      "51            ?  \n",
      "55            0  \n",
      "57            1  \n",
      "63            0  \n",
      "65            1  \n",
      "71            0  \n",
      "74            0  \n",
      "82            0  \n",
      "87            0  \n",
      "88            1  \n",
      "92            1  \n",
      "93            0  \n",
      "95            1  \n",
      "97            0  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def is_valid_ip(ip):\n",
    "    pattern = re.compile(r'^((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$')\n",
    "    return bool(pattern.match(ip))\n",
    "\n",
    "invalid_ips = data[~data['source_ip'].astype(str).apply(is_valid_ip)]\n",
    "print(invalid_ips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4281840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    log_id       source_ip destination_port protocol bytes_transferred  \\\n",
      "0       10      10.0.0.100      STRING_PORT      FTP              4096   \n",
      "34      35   192.168.1.200      STRING_PORT      FTP              4096   \n",
      "51      52    10.10.10.450      STRING_PORT      FTP              4096   \n",
      "57      58   192.168.1.475              NaN      UDP              2048   \n",
      "65      66   192.168.1.600      UNUSED_PORT      UDP              2048   \n",
      "67      68     10.10.10.77      STRING_PORT      FTP              4096   \n",
      "78      79   172.16.254.77           999999     HTTP              2048   \n",
      "97      98  192.168.1.1100      UNUSED_PORT      UDP              2048   \n",
      "\n",
      "   threat_level  \n",
      "0             ?  \n",
      "34            ?  \n",
      "51            ?  \n",
      "57            1  \n",
      "65            1  \n",
      "67            ?  \n",
      "78            1  \n",
      "97            0  \n"
     ]
    }
   ],
   "source": [
    "def is_valid_port(port):\n",
    "    try:\n",
    "        port = int(port)\n",
    "        return 0 <= port <= 65535\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "invalid_ports = data[~data['destination_port'].apply(is_valid_port)]\n",
    "print(invalid_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99b4fefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    log_id      source_ip destination_port protocol bytes_transferred  \\\n",
      "30      31  192.168.1.119              443  UNKNOWN              9513   \n",
      "80      81  192.168.1.224               25  UNKNOWN              1161   \n",
      "\n",
      "   threat_level  \n",
      "30            2  \n",
      "80            1  \n"
     ]
    }
   ],
   "source": [
    "valid_protocols = ['TCP', 'TLS', 'SSH', 'POP3', 'DNS', 'HTTPS', 'SMTP', 'FTP', 'UDP', 'HTTP']\n",
    "\n",
    "invalid_protocols = data[~data[\"protocol\"].isin(valid_protocols)]\n",
    "print(invalid_protocols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cec0892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    log_id       source_ip destination_port protocol bytes_transferred  \\\n",
      "1       12  172.16.254.100              110     POP3          NEGATIVE   \n",
      "2       27  172.16.254.200              110     POP3       NON_NUMERIC   \n",
      "93      94  192.168.1.1050               53      DNS       NON_NUMERIC   \n",
      "\n",
      "   threat_level  \n",
      "1             1  \n",
      "2             1  \n",
      "93            0  \n"
     ]
    }
   ],
   "source": [
    "def is_valid_bytes(bytes):\n",
    "    try:\n",
    "        bytes = int(bytes)\n",
    "        return bytes >= 0\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "invalid_bytes = data[~data[\"bytes_transferred\"].apply(is_valid_bytes)]\n",
    "print(invalid_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35b73600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    log_id      source_ip destination_port protocol bytes_transferred  \\\n",
      "0       10     10.0.0.100      STRING_PORT      FTP              4096   \n",
      "34      35  192.168.1.200      STRING_PORT      FTP              4096   \n",
      "51      52   10.10.10.450      STRING_PORT      FTP              4096   \n",
      "67      68    10.10.10.77      STRING_PORT      FTP              4096   \n",
      "\n",
      "   threat_level  \n",
      "0             ?  \n",
      "34            ?  \n",
      "51            ?  \n",
      "67            ?  \n"
     ]
    }
   ],
   "source": [
    "def is_valid_threat_level(threat_level):\n",
    "    try:\n",
    "        threat_level = int(threat_level)\n",
    "        return 0 <= threat_level <= 2\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "invalid_threat_levels = data[~data[\"threat_level\"].apply(is_valid_threat_level)]\n",
    "print(invalid_threat_levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90a43fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            log_id     source_ip destination_port protocol bytes_transferred  \\\n",
      "count    77.000000            77               77       77                77   \n",
      "unique         NaN            68                6        9                73   \n",
      "top            NaN  192.168.1.55               80     HTTP              1024   \n",
      "freq           NaN             3               22       22                 4   \n",
      "mean     46.519481           NaN              NaN      NaN               NaN   \n",
      "std      28.591317           NaN              NaN      NaN               NaN   \n",
      "min       1.000000           NaN              NaN      NaN               NaN   \n",
      "25%      22.000000           NaN              NaN      NaN               NaN   \n",
      "50%      45.000000           NaN              NaN      NaN               NaN   \n",
      "75%      70.000000           NaN              NaN      NaN               NaN   \n",
      "max     100.000000           NaN              NaN      NaN               NaN   \n",
      "\n",
      "       threat_level  \n",
      "count            77  \n",
      "unique            3  \n",
      "top               1  \n",
      "freq             26  \n",
      "mean            NaN  \n",
      "std             NaN  \n",
      "min             NaN  \n",
      "25%             NaN  \n",
      "50%             NaN  \n",
      "75%             NaN  \n",
      "max             NaN  \n"
     ]
    }
   ],
   "source": [
    "# Dropping invalid entries\n",
    "\n",
    "# ignore errors covers face that there might be overlap between indexes that match other invalid criteria\n",
    "data = data.drop(invalid_ips.index, errors='ignore')\n",
    "data = data.drop(invalid_ports.index, errors='ignore')\n",
    "data = data.drop(invalid_protocols.index, errors='ignore')\n",
    "data = data.drop(invalid_bytes.index, errors='ignore')\n",
    "data = data.drop(invalid_threat_levels.index, errors='ignore')\n",
    "\n",
    "print(data.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8b1395",
   "metadata": {},
   "source": [
    "Dropping preferred when accuracy paramount, and loss of some data points doesn't significantly compromise the analysis.\n",
    "Not always feasible, like if dataset is small or invalid entries contribute substantial data\n",
    "\n",
    "After dropping, only left with 77 clean entries.\n",
    "\n",
    "### Imputing Missing Values\n",
    "Replacing missing/invalid with estimated values. Maintain integrity & usability of data, especially in ML and Data Analysis tasks where missing values can lead to bias/inaccuracy\n",
    "Convert all invalid/corrupted entries like MISSING_IP,INVALID_IP,STRING_PORT,UNUSED_PORT,NON_NUMERIC, ? into NaN. Standardizes the rep of missing values, enabling uniforn downstream imputation steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0abfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from ipaddress import ip_address\n",
    "\n",
    "df = pd.read_csv('demo_dataset.csv')\n",
    "\n",
    "invalid_ips = ['INVALID_IP', 'MISSING_IP']\n",
    "invalid_ports = ['STRING_PORT', 'UNUSED_PORT']\n",
    "invalid_bytes = ['NON_NUMERIC','NEGATIVE']\n",
    "invalid_threat = ['?']\n",
    "\n",
    "df.replace(invalid_ips + invalid_ports + invalid_bytes + invalid_threat, np.nan, inplace=True)\n",
    "\n",
    "df['destination_port'] = pd.to_numeric(df['destination_port'], errors='coerce')\n",
    "df['bytes_transferred'] = pd.to_numeric(df['bytes_transferred'], errors='coerce')\n",
    "df['threat_level'] = pd.to_numeric(df['threat_level'], errors='coerce')\n",
    "\n",
    "def is_valid_ip(ip):\n",
    "    pattern = re.compile(r'^((25[0-5]|2[0-4][0-9]|[01]?\\d?\\d)\\.){3}(25[0-5]|2[0-4]\\d|[01]?\\d?\\d)$')\n",
    "    if pd.isna(ip) or not pattern.match(str(ip)):\n",
    "        return np.nan\n",
    "    return ip\n",
    "\n",
    "df['source_ip'] = df['source_ip'].apply(is_valid_ip)\n",
    "\n",
    "#NaN now represents all missing/invalid data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "313d9bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic numeric columns like bytes_transferred. Simple methods such as median/mean. For categorical like protocol, use most frequent\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "numeric_cols = ['destination_port','bytes_transferred','threat_level']\n",
    "categorical_cols = ['protocol']\n",
    "\n",
    "num_imputer = SimpleImputer(strategy='median')\n",
    "df[numeric_cols] = num_imputer.fit_transform(df[numeric_cols])\n",
    "\n",
    "cat_imputer = SimpleImputer(strategy='most_frequent')\n",
    "df[categorical_cols] = cat_imputer.fit_transform(df[categorical_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca07332",
   "metadata": {},
   "source": [
    "These imputations ensure cols have valid, non-missing vals, do not consider complex relationships among features\n",
    "\n",
    "For more sophisticated scenarios, use advanced techniques like KNNImputer or IterativeImputer. These consider relationships among features to produce contextually meaningful imputations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11b768a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "knn_imputer = KNNImputer(n_neighbors=5)\n",
    "df[numeric_cols] = knn_imputer.fit_transform(df[numeric_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b673393e",
   "metadata": {},
   "source": [
    "After cleaning & imputations, apply domain knowledge. For source_ip values that are still missing, assign a default like 0.0.0.0. Validate protocol vals against known protocols. For ports, ensure it's in range, and for protocols that imply certain ports, consider mode-based assignment or domain-specific mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f318df",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_protocols = ['TCP', 'TLS', 'SSH', 'POP3', 'DNS', 'HTTPS', 'SMTP', 'FTP', 'UDP', 'HTTP']\n",
    "df.loc[~df['protocol'].isin(valid_protocols), 'protocol'] = df['protocol'].mode()[0]\n",
    "\n",
    "df['source_ip'] = df['source_ip'].fillna('0.0.0.0')\n",
    "df['destination_port'] = df['destination_port'].clip(lower=0, upper=65535)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06f8e6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            log_id source_ip  destination_port protocol  bytes_transferred  \\\n",
      "count   100.000000       100        100.000000      100          100.00000   \n",
      "unique         NaN        76               NaN        9                NaN   \n",
      "top            NaN   0.0.0.0               NaN     HTTP                NaN   \n",
      "freq           NaN        15               NaN       27                NaN   \n",
      "mean     50.500000       NaN        776.860000      NaN         4138.64000   \n",
      "std      29.011492       NaN       6542.582099      NaN         2526.40978   \n",
      "min       1.000000       NaN         22.000000      NaN          498.00000   \n",
      "25%      25.750000       NaN         53.000000      NaN         1693.25000   \n",
      "50%      50.500000       NaN         80.000000      NaN         4096.00000   \n",
      "75%      75.250000       NaN        110.000000      NaN         5971.75000   \n",
      "max     100.000000       NaN      65535.000000      NaN         9765.00000   \n",
      "\n",
      "        threat_level  \n",
      "count     100.000000  \n",
      "unique           NaN  \n",
      "top              NaN  \n",
      "freq             NaN  \n",
      "mean        0.930000  \n",
      "std         0.781801  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         1.000000  \n",
      "75%         2.000000  \n",
      "max         2.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ed00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htb-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
